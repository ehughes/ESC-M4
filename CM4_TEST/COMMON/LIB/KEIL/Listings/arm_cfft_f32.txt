; generated by Component: ARM Compiler 5.06 update 3 (build 300) Tool: ArmCC [4d35f0]
; commandline ArmCC [--list --split_sections --debug -c --asm --interleave -o.\objects\arm_cfft_f32.o --asm_dir=.\Listings\ --list_dir=.\Listings\ --depend=.\objects\arm_cfft_f32.d --cpu=Cortex-M4.fp --apcs=interwork -O1 --diag_suppress=9931 -I..\..\SRC\CMSIS_DSP_4_5\inc -IC:\Users\emh203\Documents\GitHub\ESC-M4\CM4_TEST\COMMON\LIB\KEIL\RTE -IC:\Keil_v5\ARM\PACK\ARM\CMSIS\4.5.0\Device\ARM\ARMCM4\Include -IC:\Keil_v5\ARM\CMSIS\Include -D__UVISION_VERSION=521 -DARMCM4_FP -D__FPU_PRESENT=1 -DARM_MATH_CM4 --omf_browse=.\objects\arm_cfft_f32.crf ..\..\SRC\CMSIS_DSP_4_5\src\TransformFunctions\arm_cfft_f32.c]
                          THUMB

                          AREA ||i.arm_cfft_f32||, CODE, READONLY, ALIGN=1

                  arm_cfft_f32 PROC
;;;573    
;;;574    void arm_cfft_f32( 
000000  e92d47f0          PUSH     {r4-r10,lr}
;;;575        const arm_cfft_instance_f32 * S, 
;;;576        float32_t * p1,
;;;577        uint8_t ifftFlag,
;;;578        uint8_t bitReverseFlag)
;;;579    {
000004  4607              MOV      r7,r0
000006  460d              MOV      r5,r1
000008  4690              MOV      r8,r2
00000a  4699              MOV      r9,r3
;;;580        uint32_t  L = S->fftLen, l;
00000c  883c              LDRH     r4,[r7,#0]
;;;581        float32_t invL, * pSrc;
;;;582    
;;;583        if(ifftFlag == 1u)
00000e  f1b80f01          CMP      r8,#1
000012  d10c              BNE      |L1.46|
;;;584        {
;;;585            /*  Conjugate input data  */
;;;586            pSrc = p1 + 1;
000014  1d2e              ADDS     r6,r5,#4
;;;587            for(l=0; l<L; l++) 
000016  2000              MOVS     r0,#0
000018  e007              B        |L1.42|
                  |L1.26|
;;;588            {
;;;589                *pSrc = -*pSrc;
00001a  ed960a00          VLDR     s0,[r6,#0]
00001e  eeb10a40          VNEG.F32 s0,s0
000022  ed860a00          VSTR     s0,[r6,#0]
;;;590                pSrc += 2;
000026  3608              ADDS     r6,r6,#8
000028  1c40              ADDS     r0,r0,#1              ;587
                  |L1.42|
00002a  42a0              CMP      r0,r4                 ;587
00002c  d3f5              BCC      |L1.26|
                  |L1.46|
;;;591            }
;;;592        }
;;;593    
;;;594        switch (L) 
00002e  f5b47f80          CMP      r4,#0x100
000032  d01b              BEQ      |L1.108|
000034  dc08              BGT      |L1.72|
000036  2c10              CMP      r4,#0x10
000038  d013              BEQ      |L1.98|
00003a  2c20              CMP      r4,#0x20
00003c  d016              BEQ      |L1.108|
00003e  2c40              CMP      r4,#0x40
000040  d019              BEQ      |L1.118|
000042  2c80              CMP      r4,#0x80
000044  d11d              BNE      |L1.130|
000046  e00c              B        |L1.98|
                  |L1.72|
000048  f5b47f00          CMP      r4,#0x200
00004c  d013              BEQ      |L1.118|
00004e  f5b46f80          CMP      r4,#0x400
000052  d006              BEQ      |L1.98|
000054  f5b46f00          CMP      r4,#0x800
000058  d008              BEQ      |L1.108|
00005a  f5b45f80          CMP      r4,#0x1000
00005e  d110              BNE      |L1.130|
000060  e009              B        |L1.118|
                  |L1.98|
;;;595        {
;;;596        case 16: 
;;;597        case 128:
;;;598        case 1024:
;;;599            arm_cfft_radix8by2_f32  ( (arm_cfft_instance_f32 *) S, p1);
000062  4629              MOV      r1,r5
000064  4638              MOV      r0,r7
000066  f7fffffe          BL       arm_cfft_radix8by2_f32
;;;600            break;
00006a  e00a              B        |L1.130|
                  |L1.108|
;;;601        case 32:
;;;602        case 256:
;;;603        case 2048:
;;;604            arm_cfft_radix8by4_f32  ( (arm_cfft_instance_f32 *) S, p1);
00006c  4629              MOV      r1,r5
00006e  4638              MOV      r0,r7
000070  f7fffffe          BL       arm_cfft_radix8by4_f32
;;;605            break;
000074  e005              B        |L1.130|
                  |L1.118|
;;;606        case 64:
;;;607        case 512:
;;;608        case 4096:
;;;609            arm_radix8_butterfly_f32( p1, L, (float32_t *) S->pTwiddle, 1);
000076  b2a1              UXTH     r1,r4
000078  2301              MOVS     r3,#1
00007a  4628              MOV      r0,r5
00007c  687a              LDR      r2,[r7,#4]
00007e  f7fffffe          BL       arm_radix8_butterfly_f32
                  |L1.130|
;;;610            break;
;;;611        }  
;;;612    
;;;613        if( bitReverseFlag )
000082  f1b90f00          CMP      r9,#0
000086  d004              BEQ      |L1.146|
;;;614            arm_bitreversal_32((uint32_t*)p1,S->bitRevLength,S->pBitRevTable);
000088  89b9              LDRH     r1,[r7,#0xc]
00008a  4628              MOV      r0,r5
00008c  68ba              LDR      r2,[r7,#8]
00008e  f7fffffe          BL       arm_bitreversal_32
                  |L1.146|
;;;615    
;;;616        if(ifftFlag == 1u)
000092  f1b80f01          CMP      r8,#1
000096  d11b              BNE      |L1.208|
;;;617        {
;;;618            invL = 1.0f/(float32_t)L;
000098  ee004a10          VMOV     s0,r4
00009c  eeb71a00          VMOV.F32 s2,#1.00000000
0000a0  eef80a40          VCVT.F32.U32 s1,s0
0000a4  ee810a20          VDIV.F32 s0,s2,s1
;;;619            /*  Conjugate and scale output data */
;;;620            pSrc = p1;
;;;621            for(l=0; l<L; l++) 
0000a8  2000              MOVS     r0,#0
0000aa  e00f              B        |L1.204|
                  |L1.172|
;;;622            {
;;;623                *pSrc++ *=   invL ;
0000ac  edd50a00          VLDR     s1,[r5,#0]
0000b0  ee600a80          VMUL.F32 s1,s1,s0
0000b4  ece50a01          VSTM     r5!,{s1}
;;;624                *pSrc  = -(*pSrc) * invL;
0000b8  edd50a00          VLDR     s1,[r5,#0]
0000bc  eef10a60          VNEG.F32 s1,s1
0000c0  ee600a80          VMUL.F32 s1,s1,s0
0000c4  edc50a00          VSTR     s1,[r5,#0]
;;;625                pSrc++;
0000c8  1d2d              ADDS     r5,r5,#4
0000ca  1c40              ADDS     r0,r0,#1              ;621
                  |L1.204|
0000cc  42a0              CMP      r0,r4                 ;621
0000ce  d3ed              BCC      |L1.172|
                  |L1.208|
;;;626            }
;;;627        }
;;;628    }
0000d0  e8bd87f0          POP      {r4-r10,pc}
;;;629    
                          ENDP


                          AREA ||i.arm_cfft_radix8by2_f32||, CODE, READONLY, ALIGN=1

                  arm_cfft_radix8by2_f32 PROC
;;;206    
;;;207    void arm_cfft_radix8by2_f32( arm_cfft_instance_f32 * S, float32_t * p1) 
000000  e92d41f0          PUSH     {r4-r8,lr}
;;;208    {
000004  4606              MOV      r6,r0
000006  ed2d8b02          VPUSH    {d8}
00000a  b090              SUB      sp,sp,#0x40
;;;209        uint32_t    L  = S->fftLen;
00000c  8830              LDRH     r0,[r6,#0]
;;;210        float32_t * pCol1, * pCol2, * pMid1, * pMid2;
;;;211        float32_t * p2 = p1 + L;
00000e  eb010480          ADD      r4,r1,r0,LSL #2
;;;212        const float32_t * tw = (float32_t *) S->pTwiddle;
000012  f8d6c004          LDR      r12,[r6,#4]
;;;213        float32_t t1[4], t2[4], t3[4], t4[4], twR, twI;
;;;214        float32_t m0, m1, m2, m3;
;;;215        uint32_t l;
;;;216    
;;;217        pCol1 = p1;
000016  4688              MOV      r8,r1
;;;218        pCol2 = p2;
000018  4627              MOV      r7,r4
;;;219    
;;;220        //    Define new length
;;;221        L >>= 1;
00001a  0845              LSRS     r5,r0,#1
;;;222        //    Initialize mid pointers
;;;223        pMid1 = p1 + L;
00001c  eb010085          ADD      r0,r1,r5,LSL #2
;;;224        pMid2 = p2 + L;
000020  eb040285          ADD      r2,r4,r5,LSL #2
;;;225    
;;;226        // do two dot Fourier transform
;;;227        for ( l = L >> 2; l > 0; l-- ) 
000024  08ab              LSRS     r3,r5,#2
000026  2b00              CMP      r3,#0
000028  d07e              BEQ      |L2.296|
                  |L2.42|
;;;228        {
;;;229            t1[0] = p1[0];
00002a  ed913a00          VLDR     s6,[r1,#0]
00002e  ed8d3a0c          VSTR     s6,[sp,#0x30]
;;;230            t1[1] = p1[1];
000032  edd13a01          VLDR     s7,[r1,#4]
000036  edcd3a0d          VSTR     s7,[sp,#0x34]
;;;231            t1[2] = p1[2];
00003a  ed911a02          VLDR     s2,[r1,#8]
00003e  ed8d1a0e          VSTR     s2,[sp,#0x38]
;;;232            t1[3] = p1[3];
000042  ed916a03          VLDR     s12,[r1,#0xc]
000046  ed8d6a0f          VSTR     s12,[sp,#0x3c]
;;;233    
;;;234            t2[0] = p2[0];
00004a  edd47a00          VLDR     s15,[r4,#0]
;;;235            t2[1] = p2[1];
00004e  ed947a01          VLDR     s14,[r4,#4]
;;;236            t2[2] = p2[2];
000052  edd46a02          VLDR     s13,[r4,#8]
;;;237            t2[3] = p2[3];
000056  edd42a03          VLDR     s5,[r4,#0xc]
;;;238    
;;;239            t3[0] = pMid1[0];
00005a  edd04a00          VLDR     s9,[r0,#0]
00005e  edcd4a04          VSTR     s9,[sp,#0x10]
;;;240            t3[1] = pMid1[1];
000062  ed900a01          VLDR     s0,[r0,#4]
000066  ed8d0a05          VSTR     s0,[sp,#0x14]
;;;241            t3[2] = pMid1[2];
00006a  ed904a02          VLDR     s8,[r0,#8]
00006e  ed8d4a06          VSTR     s8,[sp,#0x18]
;;;242            t3[3] = pMid1[3];
000072  edd00a03          VLDR     s1,[r0,#0xc]
000076  edcd0a07          VSTR     s1,[sp,#0x1c]
;;;243    
;;;244            t4[0] = pMid2[0];
00007a  edd25a00          VLDR     s11,[r2,#0]
;;;245            t4[1] = pMid2[1];
00007e  edd21a01          VLDR     s3,[r2,#4]
;;;246            t4[2] = pMid2[2];
000082  ed925a02          VLDR     s10,[r2,#8]
;;;247            t4[3] = pMid2[3];
000086  ed922a03          VLDR     s4,[r2,#0xc]
;;;248    
;;;249            *p1++ = t1[0] + t2[0];
00008a  ee338a27          VADD.F32 s16,s6,s15
00008e  eca18a01          VSTM     r1!,{s16}
;;;250            *p1++ = t1[1] + t2[1];
000092  ee338a87          VADD.F32 s16,s7,s14
000096  eca18a01          VSTM     r1!,{s16}
;;;251            *p1++ = t1[2] + t2[2];
00009a  ee318a26          VADD.F32 s16,s2,s13
00009e  eca18a01          VSTM     r1!,{s16}
;;;252            *p1++ = t1[3] + t2[3];    // col 1
0000a2  ee368a22          VADD.F32 s16,s12,s5
0000a6  eca18a01          VSTM     r1!,{s16}
;;;253    
;;;254            t2[0] = t1[0] - t2[0];
0000aa  ee333a67          VSUB.F32 s6,s6,s15
0000ae  ed8d3a08          VSTR     s6,[sp,#0x20]
;;;255            t2[1] = t1[1] - t2[1];
0000b2  ee733ac7          VSUB.F32 s7,s7,s14
0000b6  edcd3a09          VSTR     s7,[sp,#0x24]
;;;256            t2[2] = t1[2] - t2[2];
0000ba  ee311a66          VSUB.F32 s2,s2,s13
0000be  ed8d1a0a          VSTR     s2,[sp,#0x28]
;;;257            t2[3] = t1[3] - t2[3];    // for col 2
0000c2  ee762a62          VSUB.F32 s5,s12,s5
0000c6  edcd2a0b          VSTR     s5,[sp,#0x2c]
;;;258    
;;;259            *pMid1++ = t3[0] + t4[0];
0000ca  ee346aa5          VADD.F32 s12,s9,s11
0000ce  eca06a01          VSTM     r0!,{s12}
;;;260            *pMid1++ = t3[1] + t4[1];
0000d2  ee306a21          VADD.F32 s12,s0,s3
0000d6  eca06a01          VSTM     r0!,{s12}
;;;261            *pMid1++ = t3[2] + t4[2];
0000da  ee346a05          VADD.F32 s12,s8,s10
0000de  eca06a01          VSTM     r0!,{s12}
;;;262            *pMid1++ = t3[3] + t4[3]; // col 1
0000e2  ee306a82          VADD.F32 s12,s1,s4
0000e6  eca06a01          VSTM     r0!,{s12}
;;;263    
;;;264            t4[0] = t4[0] - t3[0];
0000ea  ee754ae4          VSUB.F32 s9,s11,s9
0000ee  edcd4a00          VSTR     s9,[sp,#0]
;;;265            t4[1] = t4[1] - t3[1];
0000f2  ee711ac0          VSUB.F32 s3,s3,s0
0000f6  edcd1a01          VSTR     s3,[sp,#4]
;;;266            t4[2] = t4[2] - t3[2];
0000fa  ee354a44          VSUB.F32 s8,s10,s8
0000fe  ed8d4a02          VSTR     s8,[sp,#8]
;;;267            t4[3] = t4[3] - t3[3];    // for col 2
000102  ee322a60          VSUB.F32 s4,s4,s1
000106  ed8d2a03          VSTR     s4,[sp,#0xc]
;;;268    
;;;269            twR = *tw++;
00010a  ecbc0a01          VLDM     r12!,{s0}
;;;270            twI = *tw++;
00010e  ecfc0a01          VLDM     r12!,{s1}
;;;271    
;;;272            // multiply by twiddle factors
;;;273            m0 = t2[0] * twR;
000112  ee235a00          VMUL.F32 s10,s6,s0
;;;274            m1 = t2[1] * twI;
000116  ee635aa0          VMUL.F32 s11,s7,s1
;;;275            m2 = t2[1] * twR;
00011a  ee633a80          VMUL.F32 s7,s7,s0
;;;276            m3 = t2[0] * twI;
00011e  ee233a20          VMUL.F32 s6,s6,s1
;;;277            
;;;278            // R  =  R  *  Tr - I * Ti
;;;279            *p2++ = m0 + m1;
000122  ee355a25          VADD.F32 s10,s10,s11
000126  e000              B        |L2.298|
                  |L2.296|
000128  e03d              B        |L2.422|
                  |L2.298|
00012a  eca45a01          VSTM     r4!,{s10}
;;;280            // I  =  I  *  Tr + R * Ti
;;;281            *p2++ = m2 - m3;
00012e  ee333ac3          VSUB.F32 s6,s7,s6
000132  eca43a01          VSTM     r4!,{s6}
;;;282            
;;;283            // use vertical symmetry
;;;284            //  0.9988 - 0.0491i <==> -0.0491 - 0.9988i
;;;285            m0 = t4[0] * twI;
000136  ee243aa0          VMUL.F32 s6,s9,s1
;;;286            m1 = t4[1] * twR;
00013a  ee613a80          VMUL.F32 s7,s3,s0
;;;287            m2 = t4[1] * twI;
00013e  ee610aa0          VMUL.F32 s1,s3,s1
;;;288            m3 = t4[0] * twR;
000142  ee240a80          VMUL.F32 s0,s9,s0
;;;289            
;;;290            *pMid2++ = m0 - m1;
000146  ee731a63          VSUB.F32 s3,s6,s7
00014a  ece21a01          VSTM     r2!,{s3}
;;;291            *pMid2++ = m2 + m3;
00014e  ee300a80          VADD.F32 s0,s1,s0
000152  eca20a01          VSTM     r2!,{s0}
;;;292    
;;;293            twR = *tw++;
000156  ecbc0a01          VLDM     r12!,{s0}
;;;294            twI = *tw++;
00015a  ecfc0a01          VLDM     r12!,{s1}
;;;295            
;;;296            m0 = t2[2] * twR;
00015e  ee611a00          VMUL.F32 s3,s2,s0
;;;297            m1 = t2[3] * twI;
000162  ee223aa0          VMUL.F32 s6,s5,s1
;;;298            m2 = t2[3] * twR;
000166  ee622a80          VMUL.F32 s5,s5,s0
;;;299            m3 = t2[2] * twI;
00016a  ee211a20          VMUL.F32 s2,s2,s1
;;;300            
;;;301            *p2++ = m0 + m1;
00016e  ee711a83          VADD.F32 s3,s3,s6
000172  ece41a01          VSTM     r4!,{s3}
;;;302            *p2++ = m2 - m3;
000176  ee321ac1          VSUB.F32 s2,s5,s2
00017a  eca41a01          VSTM     r4!,{s2}
;;;303            
;;;304            m0 = t4[2] * twI;
00017e  ee241a20          VMUL.F32 s2,s8,s1
;;;305            m1 = t4[3] * twR;
000182  ee621a00          VMUL.F32 s3,s4,s0
;;;306            m2 = t4[3] * twI;
000186  ee620a20          VMUL.F32 s1,s4,s1
;;;307            m3 = t4[2] * twR;
00018a  ee240a00          VMUL.F32 s0,s8,s0
;;;308            
;;;309            *pMid2++ = m0 - m1;
00018e  ee311a61          VSUB.F32 s2,s2,s3
000192  eca21a01          VSTM     r2!,{s2}
;;;310            *pMid2++ = m2 + m3;
000196  ee300a80          VADD.F32 s0,s1,s0
00019a  eca20a01          VSTM     r2!,{s0}
00019e  1e5b              SUBS     r3,r3,#1              ;227
0001a0  2b00              CMP      r3,#0                 ;227
0001a2  f47faf42          BNE      |L2.42|
                  |L2.422|
;;;311        }
;;;312    
;;;313        // first col
;;;314        arm_radix8_butterfly_f32( pCol1, L, (float32_t *) S->pTwiddle, 2u);
0001a6  b2a9              UXTH     r1,r5
0001a8  2302              MOVS     r3,#2
0001aa  4640              MOV      r0,r8
0001ac  6872              LDR      r2,[r6,#4]
0001ae  f7fffffe          BL       arm_radix8_butterfly_f32
;;;315        // second col
;;;316        arm_radix8_butterfly_f32( pCol2, L, (float32_t *) S->pTwiddle, 2u);
0001b2  6872              LDR      r2,[r6,#4]
0001b4  b010              ADD      sp,sp,#0x40
0001b6  b2a9              UXTH     r1,r5
0001b8  ecbd8b02          VPOP     {d8}
0001bc  4638              MOV      r0,r7
0001be  e8bd41f0          POP      {r4-r8,lr}
0001c2  2302              MOVS     r3,#2
0001c4  f7ffbffe          B.W      arm_radix8_butterfly_f32
;;;317    }
;;;318    
                          ENDP


                          AREA ||i.arm_cfft_radix8by4_f32||, CODE, READONLY, ALIGN=1

                  arm_cfft_radix8by4_f32 PROC
;;;318    
;;;319    void arm_cfft_radix8by4_f32( arm_cfft_instance_f32 * S, float32_t * p1) 
000000  e92d4ff0          PUSH     {r4-r11,lr}
;;;320    {
000004  4680              MOV      r8,r0
000006  ed2d8b04          VPUSH    {d8-d9}
00000a  b093              SUB      sp,sp,#0x4c
;;;321        uint32_t    L  = S->fftLen >> 1;
00000c  f8b80000          LDRH     r0,[r8,#0]
000010  ea4f0c50          LSR      r12,r0,#1
;;;322        float32_t * pCol1, *pCol2, *pCol3, *pCol4, *pEnd1, *pEnd2, *pEnd3, *pEnd4;
;;;323        const float32_t *tw2, *tw3, *tw4;
;;;324        float32_t * p2 = p1 + L;
000014  eb01068c          ADD      r6,r1,r12,LSL #2
;;;325        float32_t * p3 = p2 + L;
000018  eb06008c          ADD      r0,r6,r12,LSL #2
;;;326        float32_t * p4 = p3 + L;
00001c  eb00078c          ADD      r7,r0,r12,LSL #2
;;;327        float32_t t2[4], t3[4], t4[4], twR, twI;
;;;328        float32_t p1ap3_0, p1sp3_0, p1ap3_1, p1sp3_1;
;;;329        float32_t m0, m1, m2, m3;
;;;330        uint32_t l, twMod2, twMod3, twMod4;
;;;331    
;;;332        pCol1 = p1;         // points to real values by default
000020  9111              STR      r1,[sp,#0x44]
;;;333        pCol2 = p2;
000022  9610              STR      r6,[sp,#0x40]
;;;334        pCol3 = p3;
000024  900f              STR      r0,[sp,#0x3c]
;;;335        pCol4 = p4;
000026  970e              STR      r7,[sp,#0x38]
;;;336        pEnd1 = p2 - 1;     // points to imaginary values by default
000028  1f35              SUBS     r5,r6,#4
;;;337        pEnd2 = p3 - 1;
00002a  1f02              SUBS     r2,r0,#4
;;;338        pEnd3 = p4 - 1;
00002c  1f3c              SUBS     r4,r7,#4
;;;339        pEnd4 = pEnd3 + L;
00002e  eb04038c          ADD      r3,r4,r12,LSL #2
;;;340    
;;;341        tw2 = tw3 = tw4 = (float32_t *) S->pTwiddle;
000032  f8d89004          LDR      r9,[r8,#4]
;;;342    
;;;343        L >>= 1;
000036  ea4f0c5c          LSR      r12,r12,#1
00003a  f8cdc048          STR      r12,[sp,#0x48]
;;;344    
;;;345        // do four dot Fourier transform
;;;346    
;;;347        twMod2 = 2;
;;;348        twMod3 = 4;
00003e  f04f0c04          MOV      r12,#4
000042  f8cdc004          STR      r12,[sp,#4]
;;;349        twMod4 = 6;
000046  f04f0c06          MOV      r12,#6
00004a  f8cdc000          STR      r12,[sp,#0]
;;;350    
;;;351        // TOP
;;;352        p1ap3_0 = p1[0] + p3[0];
00004e  ed910a00          VLDR     s0,[r1,#0]
000052  edd00a00          VLDR     s1,[r0,#0]
000056  ee301a20          VADD.F32 s2,s0,s1
;;;353        p1sp3_0 = p1[0] - p3[0];
00005a  ee302a60          VSUB.F32 s4,s0,s1
;;;354        p1ap3_1 = p1[1] + p3[1];
00005e  ed910a01          VLDR     s0,[r1,#4]
000062  edd00a01          VLDR     s1,[r0,#4]
000066  ee703a20          VADD.F32 s7,s0,s1
;;;355        p1sp3_1 = p1[1] - p3[1];
00006a  ee702a60          VSUB.F32 s5,s0,s1
;;;356    
;;;357        // col 2
;;;358        t2[0] = p1sp3_0 + p2[1] - p4[1];
00006e  edd61a01          VLDR     s3,[r6,#4]
000072  ed973a01          VLDR     s6,[r7,#4]
000076  ee310a82          VADD.F32 s0,s3,s4
00007a  ee705a43          VSUB.F32 s11,s0,s6
00007e  edcd5a0a          VSTR     s11,[sp,#0x28]
;;;359        t2[1] = p1sp3_1 - p2[0] + p4[0];
000082  ed960a00          VLDR     s0,[r6,#0]
000086  edd70a00          VLDR     s1,[r7,#0]
00008a  ee324ac0          VSUB.F32 s8,s5,s0
00008e  ee344a20          VADD.F32 s8,s8,s1
000092  ed8d4a0b          VSTR     s8,[sp,#0x2c]
;;;360        // col 3
;;;361        t3[0] = p1ap3_0 - p2[0] - p4[0];
000096  ee714a40          VSUB.F32 s9,s2,s0
00009a  ee744ae0          VSUB.F32 s9,s9,s1
00009e  edcd4a06          VSTR     s9,[sp,#0x18]
;;;362        t3[1] = p1ap3_1 - p2[1] - p4[1];
0000a2  ee335ae1          VSUB.F32 s10,s7,s3
0000a6  ee355a43          VSUB.F32 s10,s10,s6
0000aa  ed8d5a07          VSTR     s10,[sp,#0x1c]
;;;363        // col 4
;;;364        t4[0] = p1sp3_0 - p2[1] + p4[1];
0000ae  ee721a61          VSUB.F32 s3,s4,s3
0000b2  ee711a83          VADD.F32 s3,s3,s6
0000b6  edcd1a02          VSTR     s3,[sp,#8]
;;;365        t4[1] = p1sp3_1 + p2[0] - p4[0];
0000ba  ee302a22          VADD.F32 s4,s0,s5
0000be  ee322a60          VSUB.F32 s4,s4,s1
0000c2  ed8d2a03          VSTR     s4,[sp,#0xc]
;;;366        // col 1
;;;367        *p1++ = p1ap3_0 + p2[0] + p4[0];
0000c6  ee300a01          VADD.F32 s0,s0,s2
0000ca  ee300a20          VADD.F32 s0,s0,s1
0000ce  eca10a01          VSTM     r1!,{s0}
;;;368        *p1++ = p1ap3_1 + p2[1] + p4[1];
0000d2  ed960a01          VLDR     s0,[r6,#4]
0000d6  edd70a01          VLDR     s1,[r7,#4]
0000da  ee300a23          VADD.F32 s0,s0,s7
0000de  ee300a20          VADD.F32 s0,s0,s1
0000e2  eca10a01          VSTM     r1!,{s0}
;;;369    
;;;370        // Twiddle factors are ones
;;;371        *p2++ = t2[0];
0000e6  ece65a01          VSTM     r6!,{s11}
;;;372        *p2++ = t2[1];
0000ea  eca64a01          VSTM     r6!,{s8}
;;;373        *p3++ = t3[0];
0000ee  ece04a01          VSTM     r0!,{s9}
;;;374        *p3++ = t3[1];
0000f2  eca05a01          VSTM     r0!,{s10}
;;;375        *p4++ = t4[0];
0000f6  ece71a01          VSTM     r7!,{s3}
;;;376        *p4++ = t4[1];
0000fa  eca72a01          VSTM     r7!,{s4}
;;;377    
;;;378        tw2 += twMod2;
0000fe  f1090a08          ADD      r10,r9,#8
;;;379        tw3 += twMod3;
000102  f1090b10          ADD      r11,r9,#0x10
;;;380        tw4 += twMod4;
000106  f1090918          ADD      r9,r9,#0x18
;;;381    
;;;382        for (l = (L - 2) >> 1; l > 0; l-- ) 
00010a  f8ddc048          LDR      r12,[sp,#0x48]
00010e  f1ac0c02          SUB      r12,r12,#2
000112  ea4f0c5c          LSR      r12,r12,#1
000116  f1bc0f00          CMP      r12,#0
00011a  d07c              BEQ      |L3.534|
                  |L3.284|
;;;383        {
;;;384            // TOP
;;;385            p1ap3_0 = p1[0] + p3[0];
00011c  ed910a00          VLDR     s0,[r1,#0]
000120  edd00a00          VLDR     s1,[r0,#0]
000124  ee303a20          VADD.F32 s6,s0,s1
;;;386            p1sp3_0 = p1[0] - p3[0];
000128  ee301a60          VSUB.F32 s2,s0,s1
;;;387            p1ap3_1 = p1[1] + p3[1];
00012c  ed910a01          VLDR     s0,[r1,#4]
000130  edd00a01          VLDR     s1,[r0,#4]
000134  ee703a20          VADD.F32 s7,s0,s1
;;;388            p1sp3_1 = p1[1] - p3[1];
000138  ee701a60          VSUB.F32 s3,s0,s1
;;;389            // col 2
;;;390            t2[0] = p1sp3_0 + p2[1] - p4[1];
00013c  ed965a01          VLDR     s10,[r6,#4]
000140  edd75a01          VLDR     s11,[r7,#4]
000144  ee350a01          VADD.F32 s0,s10,s2
000148  ee304a65          VSUB.F32 s8,s0,s11
00014c  ed8d4a0a          VSTR     s8,[sp,#0x28]
;;;391            t2[1] = p1sp3_1 - p2[0] + p4[0];
000150  ed960a00          VLDR     s0,[r6,#0]
000154  edd70a00          VLDR     s1,[r7,#0]
000158  ee312ac0          VSUB.F32 s4,s3,s0
00015c  ee724a20          VADD.F32 s9,s4,s1
000160  edcd4a0b          VSTR     s9,[sp,#0x2c]
;;;392            // col 3
;;;393            t3[0] = p1ap3_0 - p2[0] - p4[0];
000164  ee332a40          VSUB.F32 s4,s6,s0
000168  ee322a60          VSUB.F32 s4,s4,s1
00016c  ed8d2a06          VSTR     s4,[sp,#0x18]
;;;394            t3[1] = p1ap3_1 - p2[1] - p4[1];
000170  ee732ac5          VSUB.F32 s5,s7,s10
000174  ee722ae5          VSUB.F32 s5,s5,s11
000178  edcd2a07          VSTR     s5,[sp,#0x1c]
;;;395            // col 4
;;;396            t4[0] = p1sp3_0 - p2[1] + p4[1];
00017c  ee311a45          VSUB.F32 s2,s2,s10
000180  ee311a25          VADD.F32 s2,s2,s11
000184  ed8d1a02          VSTR     s2,[sp,#8]
;;;397            t4[1] = p1sp3_1 + p2[0] - p4[0];
000188  ee701a21          VADD.F32 s3,s0,s3
00018c  ee711ae0          VSUB.F32 s3,s3,s1
000190  edcd1a03          VSTR     s3,[sp,#0xc]
;;;398            // col 1 - top
;;;399            *p1++ = p1ap3_0 + p2[0] + p4[0];
000194  ee300a03          VADD.F32 s0,s0,s6
000198  ee300a20          VADD.F32 s0,s0,s1
00019c  eca10a01          VSTM     r1!,{s0}
;;;400            *p1++ = p1ap3_1 + p2[1] + p4[1];
0001a0  ed960a01          VLDR     s0,[r6,#4]
0001a4  edd70a01          VLDR     s1,[r7,#4]
0001a8  ee300a23          VADD.F32 s0,s0,s7
0001ac  ee300a20          VADD.F32 s0,s0,s1
0001b0  eca10a01          VSTM     r1!,{s0}
;;;401    
;;;402            // BOTTOM
;;;403            p1ap3_1 = pEnd1[-1] + pEnd3[-1];
0001b4  ed150a01          VLDR     s0,[r5,#-4]
0001b8  ed540a01          VLDR     s1,[r4,#-4]
0001bc  ee707a20          VADD.F32 s15,s0,s1
;;;404            p1sp3_1 = pEnd1[-1] - pEnd3[-1];
0001c0  ee303a60          VSUB.F32 s6,s0,s1
;;;405            p1ap3_0 = pEnd1[0] + pEnd3[0];
0001c4  ed950a00          VLDR     s0,[r5,#0]
0001c8  edd40a00          VLDR     s1,[r4,#0]
0001cc  ee307a20          VADD.F32 s14,s0,s1
;;;406            p1sp3_0 = pEnd1[0] - pEnd3[0];
0001d0  ee708a60          VSUB.F32 s17,s0,s1
;;;407            // col 2
;;;408            t2[2] = pEnd2[0]  - pEnd4[0] + p1sp3_1;
0001d4  ed920a00          VLDR     s0,[r2,#0]
0001d8  edd30a00          VLDR     s1,[r3,#0]
0001dc  ee309a60          VSUB.F32 s18,s0,s1
0001e0  ee396a03          VADD.F32 s12,s18,s6
0001e4  ed8d6a0c          VSTR     s12,[sp,#0x30]
;;;409            t2[3] = pEnd1[0] - pEnd3[0] - pEnd2[-1] + pEnd4[-1];
0001e8  ed523a01          VLDR     s7,[r2,#-4]
0001ec  ed138a01          VLDR     s16,[r3,#-4]
0001f0  ee385ae3          VSUB.F32 s10,s17,s7
0001f4  ee756a08          VADD.F32 s13,s10,s16
0001f8  edcd6a0d          VSTR     s13,[sp,#0x34]
;;;410            // col 3
;;;411            t3[2] = p1ap3_1 - pEnd2[-1] - pEnd4[-1];
0001fc  ee375ae3          VSUB.F32 s10,s15,s7
000200  ee355a48          VSUB.F32 s10,s10,s16
000204  ed8d5a08          VSTR     s10,[sp,#0x20]
;;;412            t3[3] = p1ap3_0 - pEnd2[0]  - pEnd4[0];
000208  ee775a40          VSUB.F32 s11,s14,s0
00020c  ee755ae0          VSUB.F32 s11,s11,s1
000210  edcd5a09          VSTR     s11,[sp,#0x24]
;;;413            // col 4
;;;414            t4[2] = pEnd2[0]  - pEnd4[0]  - p1sp3_1;
000214  e000              B        |L3.536|
                  |L3.534|
000216  e09d              B        |L3.852|
                  |L3.536|
000218  ee393a43          VSUB.F32 s6,s18,s6
00021c  ed8d3a04          VSTR     s6,[sp,#0x10]
;;;415            t4[3] = pEnd4[-1] - pEnd2[-1] - p1sp3_0;
000220  ee783a63          VSUB.F32 s7,s16,s7
000224  ee733ae8          VSUB.F32 s7,s7,s17
000228  edcd3a05          VSTR     s7,[sp,#0x14]
;;;416            // col 1 - Bottom
;;;417            *pEnd1-- = p1ap3_0 + pEnd2[0] + pEnd4[0];
00022c  ee300a07          VADD.F32 s0,s0,s14
000230  ee300a20          VADD.F32 s0,s0,s1
000234  ed850a00          VSTR     s0,[r5,#0]
000238  1f2d              SUBS     r5,r5,#4
;;;418            *pEnd1-- = p1ap3_1 + pEnd2[-1] + pEnd4[-1];
00023a  ed120a01          VLDR     s0,[r2,#-4]
00023e  ed530a01          VLDR     s1,[r3,#-4]
000242  ee300a27          VADD.F32 s0,s0,s15
000246  ee300a20          VADD.F32 s0,s0,s1
00024a  ed850a00          VSTR     s0,[r5,#0]
00024e  1f2d              SUBS     r5,r5,#4
;;;419    
;;;420            // COL 2
;;;421            // read twiddle factors
;;;422            twR = *tw2++;
000250  ecba0a01          VLDM     r10!,{s0}
;;;423            twI = *tw2++;
000254  ecfa0a01          VLDM     r10!,{s1}
;;;424            // multiply by twiddle factors
;;;425            //  let    Z1 = a + i(b),   Z2 = c + i(d)
;;;426            //   =>  Z1 * Z2  =  (a*c - b*d) + i(b*c + a*d)
;;;427            
;;;428            // Top
;;;429            m0 = t2[0] * twR;
000258  ee247a00          VMUL.F32 s14,s8,s0
;;;430            m1 = t2[1] * twI;
00025c  ee647aa0          VMUL.F32 s15,s9,s1
;;;431            m2 = t2[1] * twR;
000260  ee644a80          VMUL.F32 s9,s9,s0
;;;432            m3 = t2[0] * twI;
000264  ee244a20          VMUL.F32 s8,s8,s1
;;;433            
;;;434            *p2++ = m0 + m1;
000268  ee377a27          VADD.F32 s14,s14,s15
00026c  eca67a01          VSTM     r6!,{s14}
;;;435            *p2++ = m2 - m3;
000270  ee344ac4          VSUB.F32 s8,s9,s8
000274  eca64a01          VSTM     r6!,{s8}
;;;436            // use vertical symmetry col 2
;;;437            // 0.9997 - 0.0245i  <==>  0.0245 - 0.9997i
;;;438            // Bottom
;;;439            m0 = t2[3] * twI;
000278  ee264aa0          VMUL.F32 s8,s13,s1
;;;440            m1 = t2[2] * twR;
00027c  ee664a00          VMUL.F32 s9,s12,s0
;;;441            m2 = t2[2] * twI;
000280  ee660a20          VMUL.F32 s1,s12,s1
;;;442            m3 = t2[3] * twR;
000284  ee260a80          VMUL.F32 s0,s13,s0
;;;443            
;;;444            *pEnd2-- = m0 - m1;
000288  ee344a64          VSUB.F32 s8,s8,s9
00028c  ed824a00          VSTR     s8,[r2,#0]
000290  1f12              SUBS     r2,r2,#4
;;;445            *pEnd2-- = m2 + m3;
000292  ee300a80          VADD.F32 s0,s1,s0
000296  ed820a00          VSTR     s0,[r2,#0]
00029a  1f12              SUBS     r2,r2,#4
;;;446    
;;;447            // COL 3
;;;448            twR = tw3[0];
00029c  ed9b0a00          VLDR     s0,[r11,#0]
;;;449            twI = tw3[1];
0002a0  eddb0a01          VLDR     s1,[r11,#4]
;;;450            tw3 += twMod3;
0002a4  f8dde004          LDR      lr,[sp,#4]
0002a8  eb0b0b8e          ADD      r11,r11,lr,LSL #2
;;;451            // Top
;;;452            m0 = t3[0] * twR;
0002ac  ee224a00          VMUL.F32 s8,s4,s0
;;;453            m1 = t3[1] * twI;
0002b0  ee624aa0          VMUL.F32 s9,s5,s1
;;;454            m2 = t3[1] * twR;
0002b4  ee622a80          VMUL.F32 s5,s5,s0
;;;455            m3 = t3[0] * twI;
0002b8  ee222a20          VMUL.F32 s4,s4,s1
;;;456            
;;;457            *p3++ = m0 + m1;
0002bc  ee344a24          VADD.F32 s8,s8,s9
0002c0  eca04a01          VSTM     r0!,{s8}
;;;458            *p3++ = m2 - m3;
0002c4  ee322ac2          VSUB.F32 s4,s5,s4
0002c8  eca02a01          VSTM     r0!,{s4}
;;;459            // use vertical symmetry col 3
;;;460            // 0.9988 - 0.0491i  <==>  -0.9988 - 0.0491i
;;;461            // Bottom
;;;462            m0 = -t3[3] * twR;
0002cc  eeb12a65          VNEG.F32 s4,s11
0002d0  ee222a00          VMUL.F32 s4,s4,s0
;;;463            m1 = t3[2] * twI;
0002d4  ee652a20          VMUL.F32 s5,s10,s1
;;;464            m2 = t3[2] * twR;
0002d8  ee250a00          VMUL.F32 s0,s10,s0
;;;465            m3 = t3[3] * twI;
0002dc  ee650aa0          VMUL.F32 s1,s11,s1
;;;466            
;;;467            *pEnd3-- = m0 - m1;
0002e0  ee322a62          VSUB.F32 s4,s4,s5
0002e4  ed842a00          VSTR     s4,[r4,#0]
0002e8  1f24              SUBS     r4,r4,#4
;;;468            *pEnd3-- = m3 - m2;
0002ea  ee300ac0          VSUB.F32 s0,s1,s0
0002ee  ed840a00          VSTR     s0,[r4,#0]
0002f2  1f24              SUBS     r4,r4,#4
;;;469            
;;;470            // COL 4
;;;471            twR = tw4[0];
0002f4  ed990a00          VLDR     s0,[r9,#0]
;;;472            twI = tw4[1];
0002f8  edd90a01          VLDR     s1,[r9,#4]
;;;473            tw4 += twMod4;
0002fc  f8dde000          LDR      lr,[sp,#0]
000300  eb09098e          ADD      r9,r9,lr,LSL #2
;;;474            // Top
;;;475            m0 = t4[0] * twR;
000304  ee212a00          VMUL.F32 s4,s2,s0
;;;476            m1 = t4[1] * twI;
000308  ee612aa0          VMUL.F32 s5,s3,s1
;;;477            m2 = t4[1] * twR;
00030c  ee611a80          VMUL.F32 s3,s3,s0
;;;478            m3 = t4[0] * twI;
000310  ee211a20          VMUL.F32 s2,s2,s1
;;;479            
;;;480            *p4++ = m0 + m1;
000314  ee322a22          VADD.F32 s4,s4,s5
000318  eca72a01          VSTM     r7!,{s4}
;;;481            *p4++ = m2 - m3;
00031c  ee311ac1          VSUB.F32 s2,s3,s2
000320  eca71a01          VSTM     r7!,{s2}
;;;482            // use vertical symmetry col 4
;;;483            // 0.9973 - 0.0736i  <==>  -0.0736 + 0.9973i
;;;484            // Bottom
;;;485            m0 = t4[3] * twI;
000324  ee631aa0          VMUL.F32 s3,s7,s1
;;;486            m1 = t4[2] * twR;
000328  ee231a00          VMUL.F32 s2,s6,s0
;;;487            m2 = t4[2] * twI;
00032c  ee630a20          VMUL.F32 s1,s6,s1
;;;488            m3 = t4[3] * twR;
000330  ee230a80          VMUL.F32 s0,s7,s0
;;;489            
;;;490            *pEnd4-- = m0 - m1;
000334  ee311ac1          VSUB.F32 s2,s3,s2
000338  ed831a00          VSTR     s2,[r3,#0]
00033c  1f1b              SUBS     r3,r3,#4
;;;491            *pEnd4-- = m2 + m3;
00033e  ee300a80          VADD.F32 s0,s1,s0
000342  ed830a00          VSTR     s0,[r3,#0]
000346  1f1b              SUBS     r3,r3,#4
000348  f1ac0c01          SUB      r12,r12,#1            ;382
00034c  f1bc0f00          CMP      r12,#0                ;382
000350  f47faee4          BNE      |L3.284|
                  |L3.852|
;;;492        }
;;;493    
;;;494        //MIDDLE
;;;495        // Twiddle factors are 
;;;496        //  1.0000  0.7071-0.7071i  -1.0000i  -0.7071-0.7071i
;;;497        p1ap3_0 = p1[0] + p3[0];
000354  ed910a00          VLDR     s0,[r1,#0]
000358  edd00a00          VLDR     s1,[r0,#0]
00035c  ee702a20          VADD.F32 s5,s0,s1
;;;498        p1sp3_0 = p1[0] - p3[0];
000360  ee705a60          VSUB.F32 s11,s0,s1
;;;499        p1ap3_1 = p1[1] + p3[1];
000364  ed910a01          VLDR     s0,[r1,#4]
000368  edd00a01          VLDR     s1,[r0,#4]
00036c  ee304a20          VADD.F32 s8,s0,s1
;;;500        p1sp3_1 = p1[1] - p3[1];
000370  ee303a60          VSUB.F32 s6,s0,s1
;;;501    
;;;502        // col 2
;;;503        t2[0] = p1sp3_0 + p2[1] - p4[1];
000374  edd64a01          VLDR     s9,[r6,#4]
000378  ed975a01          VLDR     s10,[r7,#4]
00037c  ee340aa5          VADD.F32 s0,s9,s11
000380  ee301a45          VSUB.F32 s2,s0,s10
;;;504        t2[1] = p1sp3_1 - p2[0] + p4[0];
000384  ed960a00          VLDR     s0,[r6,#0]
000388  edd70a00          VLDR     s1,[r7,#0]
00038c  ee731a40          VSUB.F32 s3,s6,s0
000390  ee711aa0          VADD.F32 s3,s3,s1
;;;505        // col 3
;;;506        t3[0] = p1ap3_0 - p2[0] - p4[0];
000394  ee322ac0          VSUB.F32 s4,s5,s0
000398  ee322a60          VSUB.F32 s4,s4,s1
;;;507        t3[1] = p1ap3_1 - p2[1] - p4[1];
00039c  ee743a64          VSUB.F32 s7,s8,s9
0003a0  ee733ac5          VSUB.F32 s7,s7,s10
;;;508        // col 4
;;;509        t4[0] = p1sp3_0 - p2[1] + p4[1];
0003a4  ee754ae4          VSUB.F32 s9,s11,s9
0003a8  ee744a85          VADD.F32 s9,s9,s10
;;;510        t4[1] = p1sp3_1 + p2[0] - p4[0];
0003ac  ee303a03          VADD.F32 s6,s0,s6
0003b0  ee333a60          VSUB.F32 s6,s6,s1
;;;511        // col 1 - Top
;;;512        *p1++ = p1ap3_0 + p2[0] + p4[0];
0003b4  ee300a22          VADD.F32 s0,s0,s5
0003b8  ee300a20          VADD.F32 s0,s0,s1
0003bc  ed810a00          VSTR     s0,[r1,#0]
;;;513        *p1++ = p1ap3_1 + p2[1] + p4[1];
0003c0  ed960a01          VLDR     s0,[r6,#4]
0003c4  edd70a01          VLDR     s1,[r7,#4]
0003c8  ee300a04          VADD.F32 s0,s0,s8
0003cc  ee300a20          VADD.F32 s0,s0,s1
0003d0  ed810a01          VSTR     s0,[r1,#4]
;;;514    
;;;515        // COL 2
;;;516        twR = tw2[0];
0003d4  ed9a0a00          VLDR     s0,[r10,#0]
;;;517        twI = tw2[1];
0003d8  edda0a01          VLDR     s1,[r10,#4]
;;;518    
;;;519        m0 = t2[0] * twR;
0003dc  ee612a00          VMUL.F32 s5,s2,s0
;;;520        m1 = t2[1] * twI;
0003e0  ee214aa0          VMUL.F32 s8,s3,s1
;;;521        m2 = t2[1] * twR;
0003e4  ee210a80          VMUL.F32 s0,s3,s0
;;;522        m3 = t2[0] * twI;
0003e8  ee610a20          VMUL.F32 s1,s2,s1
;;;523    
;;;524        *p2++ = m0 + m1;
0003ec  ee321a84          VADD.F32 s2,s5,s8
0003f0  ed861a00          VSTR     s2,[r6,#0]
;;;525        *p2++ = m2 - m3;
0003f4  ee300a60          VSUB.F32 s0,s0,s1
0003f8  ed860a01          VSTR     s0,[r6,#4]
;;;526        // COL 3
;;;527        twR = tw3[0];
0003fc  ed9b0a00          VLDR     s0,[r11,#0]
;;;528        twI = tw3[1];
000400  eddb0a01          VLDR     s1,[r11,#4]
;;;529    
;;;530        m0 = t3[0] * twR;
000404  ee221a00          VMUL.F32 s2,s4,s0
;;;531        m1 = t3[1] * twI;
000408  ee631aa0          VMUL.F32 s3,s7,s1
;;;532        m2 = t3[1] * twR;
00040c  ee230a80          VMUL.F32 s0,s7,s0
;;;533        m3 = t3[0] * twI;
000410  ee620a20          VMUL.F32 s1,s4,s1
;;;534    
;;;535        *p3++ = m0 + m1;
000414  ee311a21          VADD.F32 s2,s2,s3
000418  ed801a00          VSTR     s2,[r0,#0]
;;;536        *p3++ = m2 - m3;
00041c  ee300a60          VSUB.F32 s0,s0,s1
000420  ed800a01          VSTR     s0,[r0,#4]
;;;537        // COL 4
;;;538        twR = tw4[0];
000424  edd90a00          VLDR     s1,[r9,#0]
;;;539        twI = tw4[1];
000428  ed990a01          VLDR     s0,[r9,#4]
;;;540    
;;;541        m0 = t4[0] * twR;
00042c  ee241aa0          VMUL.F32 s2,s9,s1
;;;542        m1 = t4[1] * twI;
000430  ee631a00          VMUL.F32 s3,s6,s0
;;;543        m2 = t4[1] * twR;
000434  ee630a20          VMUL.F32 s1,s6,s1
;;;544        m3 = t4[0] * twI;
000438  ee240a80          VMUL.F32 s0,s9,s0
;;;545    
;;;546        *p4++ = m0 + m1;
00043c  ee311a21          VADD.F32 s2,s2,s3
000440  ed871a00          VSTR     s2,[r7,#0]
;;;547        *p4++ = m2 - m3;
000444  ee300ac0          VSUB.F32 s0,s1,s0
000448  ed870a01          VSTR     s0,[r7,#4]
;;;548    
;;;549        // first col
;;;550        arm_radix8_butterfly_f32( pCol1, L, (float32_t *) S->pTwiddle, 4u);
00044c  9812              LDR      r0,[sp,#0x48]
00044e  f8d82004          LDR      r2,[r8,#4]
000452  b281              UXTH     r1,r0
000454  2304              MOVS     r3,#4
000456  9811              LDR      r0,[sp,#0x44]
000458  f7fffffe          BL       arm_radix8_butterfly_f32
;;;551        // second col
;;;552        arm_radix8_butterfly_f32( pCol2, L, (float32_t *) S->pTwiddle, 4u);
00045c  9812              LDR      r0,[sp,#0x48]
00045e  f8d82004          LDR      r2,[r8,#4]
000462  b281              UXTH     r1,r0
000464  2304              MOVS     r3,#4
000466  9810              LDR      r0,[sp,#0x40]
000468  f7fffffe          BL       arm_radix8_butterfly_f32
;;;553        // third col
;;;554        arm_radix8_butterfly_f32( pCol3, L, (float32_t *) S->pTwiddle, 4u);
00046c  9812              LDR      r0,[sp,#0x48]
00046e  f8d82004          LDR      r2,[r8,#4]
000472  b281              UXTH     r1,r0
000474  2304              MOVS     r3,#4
000476  980f              LDR      r0,[sp,#0x3c]
000478  f7fffffe          BL       arm_radix8_butterfly_f32
;;;555        // fourth col
;;;556        arm_radix8_butterfly_f32( pCol4, L, (float32_t *) S->pTwiddle, 4u);
00047c  9812              LDR      r0,[sp,#0x48]
00047e  f8d82004          LDR      r2,[r8,#4]
000482  b281              UXTH     r1,r0
000484  980e              LDR      r0,[sp,#0x38]
000486  b013              ADD      sp,sp,#0x4c
000488  2304              MOVS     r3,#4
00048a  ecbd8b04          VPOP     {d8-d9}
00048e  e8bd4ff0          POP      {r4-r11,lr}
000492  f7ffbffe          B.W      arm_radix8_butterfly_f32
;;;557    }
;;;558    
                          ENDP


;*** Start embedded assembler ***

#line 1 "..\\..\\SRC\\CMSIS_DSP_4_5\\src\\TransformFunctions\\arm_cfft_f32.c"
	AREA ||.rev16_text||, CODE
	THUMB
	EXPORT |__asm___14_arm_cfft_f32_c_4a34056b____REV16|
#line 129 "..\\..\\SRC\\CMSIS_DSP_4_5\\inc\\core_cmInstr.h"
|__asm___14_arm_cfft_f32_c_4a34056b____REV16| PROC
#line 130

 rev16 r0, r0
 bx lr
	ENDP
	AREA ||.revsh_text||, CODE
	THUMB
	EXPORT |__asm___14_arm_cfft_f32_c_4a34056b____REVSH|
#line 144
|__asm___14_arm_cfft_f32_c_4a34056b____REVSH| PROC
#line 145

 revsh r0, r0
 bx lr
	ENDP
	AREA ||.rrx_text||, CODE
	THUMB
	EXPORT |__asm___14_arm_cfft_f32_c_4a34056b____RRX|
#line 300
|__asm___14_arm_cfft_f32_c_4a34056b____RRX| PROC
#line 301

 rrx r0, r0
 bx lr
	ENDP

;*** End   embedded assembler ***
